{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Import glob to get the files directories recursively\n",
    "import glob\n",
    "\n",
    "# Import Garbage collector interface\n",
    "import gc \n",
    "\n",
    "# Import OpenCV to transforme pictures\n",
    "import cv2\n",
    "\n",
    "# Import Time\n",
    "import time\n",
    "\n",
    "# import numpy for math calculations\n",
    "import numpy as np\n",
    "\n",
    "# Import pandas for data (csv) manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Import matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('fivethirtyeight') \n",
    "%matplotlib inline\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from skimage.color import rgb2lab, lab2rgb\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Import pytorch to build Deel Learling Models \n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data\n",
    "from torchvision.models.inception import inception_v3\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "# Import tqdm to show a smart progress meter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import warnings to hide the unnessairy warniings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_path = \"/kaggle/input/image-colorization/ab/ab/ab1.npy\"\n",
    "l_path = \"/kaggle/input/image-colorization/l/gray_scale.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_df = np.load(ab_path)[0:5000]\n",
    "L_df = np.load(l_path)[0:5000]\n",
    "dataset = (L_df,ab_df )\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lab_to_rgb(L, ab):\n",
    "    \"\"\"\n",
    "    Takes an image or a batch of images and converts from LAB space to RGB\n",
    "    \"\"\"\n",
    "    L = L  * 100\n",
    "    ab = (ab - 0.5) * 128 * 2\n",
    "    Lab = torch.cat([L, ab], dim=2).numpy()\n",
    "    rgb_imgs = []\n",
    "    for img in Lab:\n",
    "        img_rgb = lab2rgb(img)\n",
    "        rgb_imgs.append(img_rgb)\n",
    "    return np.stack(rgb_imgs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,30))\n",
    "for i in range(1,16,2):\n",
    "    plt.subplot(4,4,i)\n",
    "    img = np.zeros((224,224,3))\n",
    "    img[:,:,0] = L_df[i]\n",
    "    plt.title('B&W')\n",
    "    plt.imshow(lab2rgb(img))\n",
    "    \n",
    "    plt.subplot(4,4,i+1)\n",
    "    img[:,:,1:] = ab_df[i]\n",
    "    img = img.astype('uint8')\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_LAB2RGB)\n",
    "    plt.title('Colored')\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageColorizationDataset(Dataset):\n",
    "    ''' Black and White (L) Images and corresponding A&B Colors'''\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        '''\n",
    "        :param dataset: Dataset name.\n",
    "        :param data_dir: Directory with all the images.\n",
    "        :param transform: Optional transform to be applied on sample\n",
    "        '''\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        L = np.array(dataset[0][idx]).reshape((224,224,1))\n",
    "        L = transforms.ToTensor()(L)\n",
    "        \n",
    "        ab = np.array(dataset[1][idx])\n",
    "        ab = transforms.ToTensor()(ab)\n",
    "\n",
    "        return ab, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "# Prepare the Datasets\n",
    "train_dataset = ImageColorizationDataset(dataset = (L_df, ab_df))\n",
    "test_dataset = ImageColorizationDataset(dataset = (L_df, ab_df))\n",
    "\n",
    "# Build DataLoaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle = True, pin_memory = True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle = False, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,out_channels,kernel_size=3, padding=1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels,kernel_size=3,padding=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.identity_map = nn.Conv2d(in_channels, out_channels,kernel_size=1,stride=stride)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    def forward(self, inputs):\n",
    "        x = inputs.clone().detach()\n",
    "        out = self.layer(x)\n",
    "        residual  = self.identity_map(inputs)\n",
    "        skip = out + residual\n",
    "        return self.relu(skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownSampleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.layer(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpSampleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.res_block = ResBlock(in_channels + out_channels, out_channels)\n",
    "        \n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.upsample(inputs)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.res_block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_channel, output_channel, dropout_rate = 0.2):\n",
    "        super().__init__()\n",
    "        self.encoding_layer1_ = ResBlock(input_channel,64)\n",
    "        self.encoding_layer2_ = DownSampleConv(64, 128)\n",
    "        self.encoding_layer3_ = DownSampleConv(128, 256)\n",
    "        self.bridge = DownSampleConv(256, 512)\n",
    "        self.decoding_layer3_ = UpSampleConv(512, 256)\n",
    "        self.decoding_layer2_ = UpSampleConv(256, 128)\n",
    "        self.decoding_layer1_ = UpSampleConv(128, 64)\n",
    "        self.output = nn.Conv2d(64, output_channel, kernel_size=1)\n",
    "        self.dropout = nn.Dropout2d(dropout_rate)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        ###################### Enocoder #########################\n",
    "        e1 = self.encoding_layer1_(inputs)\n",
    "        e1 = self.dropout(e1)\n",
    "        e2 = self.encoding_layer2_(e1)\n",
    "        e2 = self.dropout(e2)\n",
    "        e3 = self.encoding_layer3_(e2)\n",
    "        e3 = self.dropout(e3)\n",
    "        \n",
    "        ###################### Bridge #########################\n",
    "        bridge = self.bridge(e3)\n",
    "        bridge = self.dropout(bridge)\n",
    "        \n",
    "        ###################### Decoder #########################\n",
    "        d3 = self.decoding_layer3_(bridge, e3)\n",
    "        d2 = self.decoding_layer2_(d3, e2)\n",
    "        d1 = self.decoding_layer1_(d2, e1)\n",
    "        \n",
    "        ###################### Output #########################\n",
    "        output = self.output(d1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Generator(1,2).to(device)\n",
    "summary(model, (1, 224, 224), batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        def critic_block(in_filters, out_filters, normalization=True):\n",
    "            \"\"\"Returns layers of each critic block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *critic_block(in_channels, 64, normalization=False),\n",
    "            *critic_block(64, 128),\n",
    "            *critic_block(128, 256),\n",
    "            *critic_block(256, 512),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, ab, l):\n",
    "        # Concatenate image and condition image by channels to produce input\n",
    "        img_input = torch.cat((ab, l), 1)\n",
    "        output = self.model(img_input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Critic(3).to(device)\n",
    "summary(model, [(2, 224, 224), (1, 224, 224)], batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch\n",
    "def _weights_init(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_progress(cond, real, fake, current_epoch = 0, figsize=(20,15)):\n",
    "    \"\"\"\n",
    "    Save cond, real (original) and generated (fake)\n",
    "    images in one panel \n",
    "    \"\"\"\n",
    "    cond = cond.detach().cpu().permute(1, 2, 0)   \n",
    "    real = real.detach().cpu().permute(1, 2, 0)\n",
    "    fake = fake.detach().cpu().permute(1, 2, 0)\n",
    "    \n",
    "    images = [cond, real, fake]\n",
    "    titles = ['input','real','generated']\n",
    "    print(f'Epoch: {current_epoch}')\n",
    "    fig, ax = plt.subplots(1, 3, figsize=figsize)\n",
    "    for idx,img in enumerate(images):\n",
    "        if idx == 0:\n",
    "            ab = torch.zeros((224,224,2))\n",
    "            img = torch.cat([images[0]* 100, ab], dim=2).numpy()\n",
    "            imgan = lab2rgb(img)\n",
    "        else:\n",
    "            imgan = lab_to_rgb(images[0],img)\n",
    "        ax[idx].imshow(imgan)\n",
    "        ax[idx].axis(\"off\")\n",
    "    for idx, title in enumerate(titles):    \n",
    "        ax[idx].set_title('{}'.format(title))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CWGAN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, learning_rate=0.0002, lambda_recon=100, display_step=10, lambda_gp=10, lambda_r1=10,):\n",
    "\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.display_step = display_step\n",
    "        \n",
    "        self.generator = Generator(in_channels, out_channels)\n",
    "        self.critic = Critic(in_channels + out_channels)\n",
    "        self.optimizer_G = optim.Adam(self.generator.parameters(), lr=learning_rate, betas=(0.5, 0.9))\n",
    "        self.optimizer_C = optim.Adam(self.critic.parameters(), lr=learning_rate, betas=(0.5, 0.9))\n",
    "        self.lambda_recon = lambda_recon\n",
    "        self.lambda_gp = lambda_gp\n",
    "        self.lambda_r1 = lambda_r1\n",
    "        self.recon_criterion = nn.L1Loss()\n",
    "        self.generator_losses, self.critic_losses  =[],[]\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return [self.optimizer_C, self.optimizer_G]\n",
    "        \n",
    "    def generator_step(self, real_images, conditioned_images):\n",
    "        # WGAN has only a reconstruction loss\n",
    "        self.optimizer_G.zero_grad()\n",
    "        fake_images = self.generator(conditioned_images)\n",
    "        recon_loss = self.recon_criterion(fake_images, real_images)\n",
    "        recon_loss.backward()\n",
    "        self.optimizer_G.step()\n",
    "        \n",
    "        # Keep track of the average generator loss\n",
    "        self.generator_losses += [recon_loss.item()]\n",
    "        \n",
    "        \n",
    "    def critic_step(self, real_images, conditioned_images):\n",
    "        self.optimizer_C.zero_grad()\n",
    "        fake_images = self.generator(conditioned_images)\n",
    "        fake_logits = self.critic(fake_images, conditioned_images)\n",
    "        real_logits = self.critic(real_images, conditioned_images)\n",
    "        \n",
    "        # Compute the loss for the critic\n",
    "        loss_C = real_logits.mean() - fake_logits.mean()\n",
    "\n",
    "        # Compute the gradient penalty\n",
    "        alpha = torch.rand(real_images.size(0), 1, 1, 1, requires_grad=True)\n",
    "        alpha = alpha.to(device)\n",
    "        interpolated = (alpha * real_images + (1 - alpha) * fake_images.detach()).requires_grad_(True)\n",
    "        \n",
    "        interpolated_logits = self.critic(interpolated, conditioned_images)\n",
    "        \n",
    "        grad_outputs = torch.ones_like(interpolated_logits, dtype=torch.float32, requires_grad=True)\n",
    "        gradients = torch.autograd.grad(outputs=interpolated_logits, inputs=interpolated, grad_outputs=grad_outputs,create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "        \n",
    "        gradients = gradients.view(len(gradients), -1)\n",
    "        gradients_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        loss_C += self.lambda_gp * gradients_penalty\n",
    "        \n",
    "        # Compute the R1 regularization loss\n",
    "        r1_reg = gradients.pow(2).sum(1).mean()\n",
    "        loss_C += self.lambda_r1 * r1_reg\n",
    "\n",
    "        # Backpropagation\n",
    "        loss_C.backward()\n",
    "        self.optimizer_C.step()\n",
    "        self.critic_losses += [loss_C.item()]\n",
    "        \n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        real, condition = batch\n",
    "        if optimizer_idx == 0:\n",
    "            self.critic_step(real, condition)\n",
    "        elif optimizer_idx == 1:\n",
    "            self.generator_step(real, condition)\n",
    "        gen_mean = sum(self.generator_losses[-self.display_step:]) / self.display_step\n",
    "        crit_mean = sum(self.critic_losses[-self.display_step:]) / self.display_step\n",
    "        if self.current_epoch%self.display_step==0 and batch_idx==0 and optimizer_idx==1:\n",
    "            fake = self.generator(condition).detach()\n",
    "            torch.save(cwgan.generator.state_dict(), \"ResUnet_\"+ str(self.current_epoch) +\".pt\")\n",
    "            torch.save(cwgan.critic.state_dict(), \"PatchGAN_\"+ str(self.current_epoch) +\".pt\")\n",
    "            print(f\"Epoch {self.current_epoch} : Generator loss: {gen_mean}, Critic loss: {crit_mean}\")\n",
    "            display_progress(condition[0], real[0], fake[0], self.current_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "cwgan = CWGAN(in_channels = 1, out_channels = 2 ,learning_rate=2e-4, lambda_recon=100, display_step=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=150, gpus=-1)\n",
    "trainer.fit(cwgan, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,60))\n",
    "idx =1\n",
    "for batch_idx, batch in enumerate(test_loader):\n",
    "    real, condition = batch\n",
    "    pred = cwgan.generator(condition).detach().squeeze().permute(1, 2, 0)\n",
    "    condition  = condition.detach().squeeze(0).permute(1, 2, 0)\n",
    "    real  = real.detach().squeeze(0).permute(1, 2, 0)\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.subplot(6,3,idx)\n",
    "    plt.grid(False)\n",
    "    \n",
    "    ab = torch.zeros((224,224,2))\n",
    "    img = torch.cat([condition * 100, ab], dim=2).numpy()\n",
    "    imgan = lab2rgb(img)\n",
    "    plt.imshow(imgan)\n",
    "    plt.title('Input')\n",
    "    \n",
    "    plt.subplot(6,3,idx + 1)\n",
    "    \n",
    "    ab = torch.zeros((224,224,2))\n",
    "    imgan = lab_to_rgb(condition,real)\n",
    "    plt.imshow(imgan)\n",
    "    plt.title('Real')\n",
    "    \n",
    "    plt.subplot(6,3,idx + 2)\n",
    "    imgan = lab_to_rgb(condition,pred)\n",
    "    plt.title('Generated')\n",
    "    plt.imshow(imgan)\n",
    "    idx += 3\n",
    "    if idx >= 18:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable grads + batchnorm + dropout\n",
    "torch.set_grad_enabled(False)\n",
    "cwgan.generator.eval()\n",
    "all_preds = []\n",
    "all_real = []\n",
    "\n",
    "for batch_idx, batch in enumerate(test_loader):\n",
    "    real, condition = batch\n",
    "    pred = cwgan.generator(condition).detach()\n",
    "    Lab = torch.cat([condition, pred], dim=1).numpy()\n",
    "    Lab_real = torch.cat([condition, real], dim=1).numpy()\n",
    "    all_preds.append(Lab.squeeze())\n",
    "    all_real.append(Lab_real.squeeze())\n",
    "    if batch_idx == 500: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionScore:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self.inception = inception_v3(pretrained=True, transform_input=False).to(self.device)\n",
    "        self.inception.eval()\n",
    "\n",
    "    def calculate_is(self, generated_images):\n",
    "        generated_images = generated_images.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_features = self.inception(generated_images.view(-1,3,224,224))\n",
    "\n",
    "        generated_features = generated_features.view(generated_features.size(0), -1)\n",
    "        p = F.softmax(generated_features, dim=1)\n",
    "\n",
    "        kl = p * (torch.log(p) - torch.log(torch.tensor(1.0/generated_features.size(1)).to(self.device)))\n",
    "        kl = kl.sum(dim=1)\n",
    "\n",
    "        return kl.mean().item(), kl.std().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the InceptionScore class\n",
    "device = \"cuda\" # or \"cpu\" if you don't have a GPU\n",
    "is_calculator = InceptionScore(device)\n",
    "\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_preds = torch.tensor(all_preds).float()\n",
    "\n",
    "all_real = np.concatenate(all_real, axis=0)\n",
    "all_real = torch.tensor(all_real).float()\n",
    "\n",
    "is_model = InceptionScore(device)\n",
    "\n",
    "# Calculate the Inception Score\n",
    "mean_real, std_real = is_model.calculate_is(all_real)\n",
    "print(\"Inception Score of real images: mean: {:.4f}, std: {:.4f}\".format(mean_real, std_real))\n",
    "mean_is, std_is = is_model.calculate_is(all_preds)\n",
    "print(\"Inception Score of fake images: mean: {:.4f}, std: {:.4f}\".format(mean_is, std_is))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FID:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self.inception = inception_v3(pretrained=True, transform_input=False).to(self.device)\n",
    "        self.inception.eval()\n",
    "        self.mu = None\n",
    "        self.sigma = None\n",
    "\n",
    "    def calculate_fid(self, real_images, generated_images):\n",
    "        real_images = real_images.to(self.device)\n",
    "        generated_images = generated_images.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            real_features = self.inception(real_images.view(-1,3,224,224))\n",
    "            generated_features = self.inception(generated_images.view(-1,3,224,224))\n",
    "\n",
    "        real_features = real_features.view(real_features.size(0), -1)\n",
    "        generated_features = generated_features.view(generated_features.size(0), -1)\n",
    "\n",
    "        if self.mu is None:\n",
    "            self.mu = real_features.mean(dim=0)\n",
    "\n",
    "        if self.sigma is None:\n",
    "            self.sigma = real_features.std(dim=0)\n",
    "\n",
    "        real_mu = real_features.mean(dim=0)\n",
    "        real_sigma = real_features.std(dim=0)\n",
    "\n",
    "        generated_mu = generated_features.mean(dim=0)\n",
    "        generated_sigma = generated_features.std(dim=0)\n",
    "\n",
    "        mu_diff = real_mu - generated_mu\n",
    "        sigma_diff = real_sigma - generated_sigma\n",
    "\n",
    "        fid = mu_diff.pow(2).sum() + (self.sigma - generated_sigma).pow(2).sum() + (self.mu - generated_mu).pow(2).sum()\n",
    "        return fid.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the FID class\n",
    "device = \"cuda\" # or \"cpu\" if you don't have a GPU\n",
    "fid_calculator = FID(device)\n",
    "\n",
    "# Calculate the FID\n",
    "fid_value = fid_calculator.calculate_fid(all_real, all_preds)\n",
    "print(\"FID: {:.4f}\".format(fid_value))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
